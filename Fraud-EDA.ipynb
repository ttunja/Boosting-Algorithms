{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1972059f-7d5d-43a6-a2ee-2e235491e0ff",
    "_uuid": "de8b6d15-5ee0-4703-a5ad-4d605a142dca"
   },
   "source": [
    "# **Fraud Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c88f9c2-f738-4a96-b142-fcb8b2436954",
    "_uuid": "3eccb0e3-496f-45c2-a340-b394f3ec6965"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b00b20b-809d-42f0-bf7b-7720170d544f",
    "_uuid": "855c7019-3a30-4ee4-92b5-ee7b9334d1d5"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6002a282-5026-42a6-9331-f54d122dfb42",
    "_uuid": "fa14eb7b-09d9-4d09-b7f8-b52178798175"
   },
   "outputs": [],
   "source": [
    "#folder_path = '../input/ieee-fraud-detection/'\n",
    "ind = 'TransactionID'\n",
    "train_identity = pd.read_csv('train_identity.csv')\n",
    "train_transaction = pd.read_csv('train_transaction.csv')\n",
    "test_identity = pd.read_csv('test_identity.csv')\n",
    "test_transaction = pd.read_csv('test_transaction.csv')\n",
    "\n",
    "train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "45d2d52e-7d21-43da-b02a-3e8815daef7d",
    "_uuid": "752741da-2128-454e-bc43-5564cc088f5c"
   },
   "source": [
    "Data is splitted in two data sets called Identity and Transaction. Each of these two sets has a column named TransactionID. We merged our two data sets on that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "415b8e59-5b5a-44f9-8f0d-06196ebb8655",
    "_uuid": "d018a41c-e9c3-4c04-a3a2-a37449af4461"
   },
   "outputs": [],
   "source": [
    "train_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc3175a1-e487-48eb-bf13-bd0fa2d1488b",
    "_uuid": "e6df4b12-493f-4cfc-8a64-d6c5359eed24"
   },
   "outputs": [],
   "source": [
    "train_identity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "train_identity.shape, train_transaction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "\n",
    "In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n",
    "\n",
    "The data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n",
    "\n",
    "> Note: Not all transactions have corresponding identity information.\n",
    "\n",
    "**Categorical Features - Transaction**\n",
    "\n",
    "- ProductCD\n",
    "- emaildomain\n",
    "- card1 - card6\n",
    "- addr1, addr2\n",
    "- P_emaildomain\n",
    "- R_emaildomain\n",
    "- M1 - M9\n",
    "\n",
    "**Categorical Features - Identity**\n",
    "\n",
    "- DeviceType\n",
    "- DeviceInfo\n",
    "- id_12 - id_38\n",
    "\n",
    "**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n",
    "\n",
    "**Files**\n",
    "\n",
    "- train_{transaction, identity}.csv - the training set\n",
    "- test_{transaction, identity}.csv - the test set (**you must predict the isFraud value for these observations**)\n",
    "- sample_submission.csv - a sample submission file in the correct format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "591b2102-06cc-4e45-bb2d-b99d53fffd54",
    "_uuid": "06efe6d3-1780-4c02-9e18-b5b5efd93613"
   },
   "outputs": [],
   "source": [
    "print('train_transaction has shape',train_transaction.shape)\n",
    "print('train_identity has shape',train_identity.shape)\n",
    "print('test_transaction has shape',test_transaction.shape)\n",
    "print('test_identity has shape',test_identity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see **test dataset** and **train dataset** have similar number of samples. We will deal with a lot of columns in transaction dataset that have many unknown values and many features that we dont know what they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important\n",
    "> **The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n",
    "\n",
    "[The timespan of the dataset is 1 year ?\n",
    "](https://www.kaggle.com/c/ieee-fraud-detection/discussion/100071#latest-577632) by Suchith**\n",
    "\n",
    "```\n",
    "Train: min = 86400 max = 15811131\n",
    "Test: min = 18403224 max = 34214345\n",
    "```\n",
    "\n",
    "The difference train.min() and test.max() is ```x = 34214345 - 86400 = 34127945``` but we don't know is it in seconds,minutes or hours.\n",
    "\n",
    "```\n",
    "Time span of the total dataset is 394.9993634259259 days\n",
    "Time span of Train dataset is  181.99920138888888 days\n",
    "Time span of Test dataset is  182.99908564814814 days\n",
    "The gap between train and test is 30.00107638888889 days\n",
    "```\n",
    "\n",
    "If it is in seconds then dataset timespan will be ```x/(3600*24*365) = 1.0821``` years which seems reasonable to me. So if the **transactionDT** is in **seconds** then\n",
    "\n",
    "```\n",
    "Time span of the total dataset is 394.9993634259259 days\n",
    "Time span of Train dataset is  181.99920138888888 days\n",
    "Time span of Test dataset is  182.99908564814814 days\n",
    "The gap between train and test is 30.00107638888889 days\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction['TransactionDT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,8))\n",
    "ax = fig.gca()\n",
    "\n",
    "plt.hist(train['TransactionDT'], label='train',bins=100);\n",
    "plt.hist(test['TransactionDT'], label='test',bins=100);\n",
    "plt.legend();\n",
    "plt.title('Distribution of Transaction Dates');\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the assumption is right that we got data for 394 days, more precisely for a year and 30 days in between, then we can see spikes in the same part of the year.\n",
    "With help of discussions, it would seem that data set starts wit 01.12. and ends with 31.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_identity,train_transaction,test_identity,test_transaction\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the memory of numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_mem_usage(df, verbose=True):\n",
    "    num = ['int16','int32','int64','float16','float32','float64']\n",
    "    start_mem = df.memory_usage(deep = True).sum()/1024**2\n",
    "    for col in df.columns:\n",
    "        col_types = df[col].dtypes\n",
    "        if col_types in num:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_types)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem=df.memory_usage(deep = True).sum()/1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_mem_usage(train)\n",
    "red_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our data set! First thing that we are going to do is inspect how many missing and unique values are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize(df):\n",
    "    analysis = pd.DataFrame(df.dtypes,columns=['d_types'])\n",
    "    analysis = analysis.reset_index()\n",
    "    analysis = analysis.rename(columns={\"index\": \"Col_name\"})\n",
    "    analysis['Missing_values'] = df.isnull().sum().values\n",
    "    analysis['Unique_values'] = df.nunique().values\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick overview of a whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overview(df):\n",
    "    for col, values in df.iteritems():\n",
    "        num_uniques = values.nunique()\n",
    "        print ('{name}: {num_unique}'.format(name=col, num_unique=num_uniques))\n",
    "        print (values.unique())\n",
    "        print ('\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect Transaction amounts first and see if transaction amount has anything to do with fraud. My assumption is that bigger the transaction amount the bigger the chance there is that the transaction is a fraud, but on the other hand maybe we can make a different argument. Intuitively speaking, we can think that maybe someone would make a small fraud because of the fear of getting caught. Data will say more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['TransactionAmt']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the result of a describe method wasn't exactly as we would liked it to be because there is no mean and std results. That happened because dtype of a column is float16. So we do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['TransactionAmt']].astype('float32').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "plt.suptitle('Transaction Amount Distributions', fontsize=18)\n",
    "\n",
    "plt.subplot(221)\n",
    "d = sns.distplot(train['TransactionAmt'])\n",
    "d.set_title(\"Transaction Amount Distribuition\", fontsize=18)\n",
    "d.set_xlabel(\"\")\n",
    "d.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "d1 = sns.distplot(train[train['TransactionAmt'] <= 1000]['TransactionAmt'])\n",
    "d1.set_title(\"Transaction Amount Distribuition <= 1000\", fontsize=18)\n",
    "d1.set_xlabel(\"\")\n",
    "d1.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "plt.subplot(223)\n",
    "l = sns.distplot(np.log(train['TransactionAmt']),color='r')\n",
    "l.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\n",
    "l.set_xlabel(\"\")\n",
    "l.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "plt.subplot(224)\n",
    "l1 = sns.distplot(np.log(train[train['TransactionAmt']<=1000]['TransactionAmt']),color='r')\n",
    "l1.set_title(\"Transaction Amount (Log) Distribuition <= 1000\", fontsize=18)\n",
    "l1.set_xlabel(\"\")\n",
    "l1.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "plt.figure(figsize=(16,12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.suptitle('Transaction Amount Distributions, isFraud==1', fontsize=18)\n",
    "\n",
    "plt.subplot(221)\n",
    "d = sns.distplot(train[train['isFraud']==1]['TransactionAmt'])\n",
    "d.set_title(\"Transaction Amount Distribuition\", fontsize=18)\n",
    "d.set_xlabel(\"\")\n",
    "d.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "plt.subplot(222)\n",
    "d = sns.distplot(np.log(train[train['isFraud']==1]['TransactionAmt']),color='r')\n",
    "d.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\n",
    "d.set_xlabel(\"\")\n",
    "d.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "plt.suptitle('Transaction Amount Distributions, isFraud==0', fontsize=18)\n",
    "\n",
    "plt.subplot(221)\n",
    "d = sns.distplot(train[train['isFraud']==0]['TransactionAmt'],color='b')\n",
    "d.set_title(\"Transaction Amount Distribuition\", fontsize=18)\n",
    "d.set_xlabel(\"\")\n",
    "d.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "plt.subplot(222)\n",
    "d = sns.distplot(np.log(train[train['isFraud']==0]['TransactionAmt']),color='r')\n",
    "d.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\n",
    "d.set_xlabel(\"\")\n",
    "d.set_ylabel(\"Probability\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['isFraud']==0]['TransactionAmt'].values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['isFraud']==1]['TransactionAmt'].values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['isFraud']==0]['isFraud'].count()/train['isFraud'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the code above, data is imbalanced. There is 96.5% data samples with no fraud and only 3.5% with fraud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dist1, dist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know what exactly this 2 numerical features are. They could be distances between billing address, zip code, IP address, phone area..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analize(train[['dist1','dist2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:.2f}\".format((352271/590541)*100),'% of missing values in dist1 column and',\"{0:.2f}\".format((552913/590541)*100),'% of missing values in dist 2 column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "plt.suptitle('dist1 and dist2',fontsize=18)\n",
    "\n",
    "a=train[['dist1']].dropna(axis=0)\n",
    "b=train[['dist2']].dropna(axis=0)\n",
    "\n",
    "plt.subplot(221)\n",
    "d = sns.distplot(a,color='b')\n",
    "d.set_title(\"dist1 Distribuition\", fontsize=18)\n",
    "d.set_xlabel(\"\")\n",
    "d.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "plt.subplot(222)\n",
    "d2 = sns.distplot(b,color='r')\n",
    "d2.set_title(\"dist2 Distribution\", fontsize=18)\n",
    "d2.set_xlabel(\"\")\n",
    "d2.set_ylabel(\"Probability\", fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C1-C14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In discussion described as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
    "“Can you please give more examples of counts in the variables C1-15? Would these be like counts of phone numbers, email addresses, names associated with the user? I can't think of 15.\n",
    "Your guess is good, plus like device, ipaddr, billingaddr, etc. Also these are for both purchaser and recipient, which doubles the number.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analize(train[['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview(train[['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']].astype('float32').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D1-D15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real meaning behind D and C features is masked and it is hard to find a real meaning of each feature. We can only take a guess for some of the feature's meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> D1-D15: timedelta, such as days between previous transaction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analize(train[['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of missing values. Is there any reason behind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see some patterns (in most cases):\n",
    "* When D1==0 D2==Nan and when D1==x, x>0, D2==x\n",
    "* Same thing with D3 and D5, but happens less often\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']].astype('float32').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the kernel:\n",
    "\n",
    "[EDA what's behind D features?](https://www.kaggle.com/akasyanama13/eda-what-s-behind-d-features)\n",
    "\n",
    "We can see that D3 feature indicates days from the previous transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1-V339"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
    "“For example, how many times the payment card associated with a IP and email or address appeared in 24 hours time range, etc.”\n",
    "\"All Vesta features were derived as numerical. some of them are count of orders within a clustering, a time-period or condition, so the value is finite and has ordering (or ranking). I wouldn't recommend to treat any of them as categorical. If any of them resulted in binary by chance, it maybe worth trying.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',400)\n",
    "v_col = [c for c in train if c[0] == 'V']\n",
    "train[v_col].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of ones and Nan's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### id1-id11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> “id01 to id11 are numerical features for identity, which is collected by Vesta and security partners such as device rating, ip_domain rating, proxy rating, etc. Also it recorded behavioral fingerprint like account login times/failed to login times, how long an account stayed on the page, etc. All of these are not able to elaborate due to security partner T&C. I hope you could get basic meaning of these features, and by mentioning them as numerical/categorical, you won't deal with them inappropriately.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = ['id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10','id_11']\n",
    "train[id_col].astype('float32').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing that is bugging me is why are there so many negative values, because by the description of the Vesta company this features seem all to be positive. Let try to analize some general stuff and aybe come to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analize(train[id_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(35, 12))\n",
    "features = list(train[id_col])\n",
    "uniques = [len(train[col].unique()) for col in features]\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.barplot(features, uniques, log=True)\n",
    "ax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\n",
    "for p, uniq in zip(ax.patches, uniques):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 10,\n",
    "            uniq,\n",
    "            ha=\"center\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(39):\n",
    "    if i<9:\n",
    "        test=test.rename(columns={\"id-0\"+str(i+1): \"id_0\"+str(i+1)})\n",
    "    test=test.rename(columns={\"id-\"+str(i+1): \"id_\"+str(i+1)})\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(35, 12))\n",
    "features = list(test[id_col])\n",
    "uniques = [len(test[col].unique()) for col in features]\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.barplot(features, uniques, log=True)\n",
    "ax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TEST')\n",
    "for p, uniq in zip(ax.patches, uniques):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 10,\n",
    "            uniq,\n",
    "            ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both in test set and training set column with the highest number of unique values is id_02. We have less missing values in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start analyzing categorical features. We will start with id categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(12,39):\n",
    "    l.append('id_'+str(i))\n",
    "train[l].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=[i for i in l if train[i].dtype=='float16']\n",
    "train[n]=train[n].astype('float32')\n",
    "train[n].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [k for k in l if train[k].dtype=='object']\n",
    "train[c].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analize(train[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features have a lot of unique values (look more like numerical data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview(train[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in n:\n",
    "    try:\n",
    "        train.set_index('TransactionDT')[i].plot(style='.', title=i, figsize=(15, 3))\n",
    "        test.set_index('TransactionDT')[i].plot(style='.', title=i, figsize=(15, 3))\n",
    "        plt.show()\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some repetative behaviour (first month resembles the last one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['TransactionDT'] + n\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(train[cols].corr(), cmap='RdBu_r', annot=True, center=0.0)\n",
    "plt.title('ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode all categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_c = ['id_12','id_15','id_16','id_27','id_28','id_29','id_34','id_35','id_36','id_37','id_38']\n",
    "nenc_c=[k for k in c if k not in enc_c]\n",
    "dc = {'Unknown':-1,'NotFound':0,'Found':1,'New':2,'F':0,'T':1,'match_status:2':2, 'match_status:1':1, 'match_status:0':0, 'match_status:-1':-1}\n",
    "for i in enc_c:\n",
    "    train[i]=train[i].map(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['TransactionDT'] + enc_c\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(train[cols].corr(), cmap='RdBu_r', annot=True, center=0.0)\n",
    "plt.title('ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nenc_c:\n",
    "    plt.figure(figsize=(80,30))\n",
    "\n",
    "    train[i]=train[i].fillna('Missing')\n",
    "    features = list(train[i].unique()[:20])\n",
    "    #if you want to see 10 most frequent values \n",
    "    #features = train['DeviceInfo'].value_counts()[:10].index.tolist()\n",
    "    uniques = [(train[i]==col).sum() for col in features]\n",
    "    sns.set(font_scale=2)\n",
    "    ax = sns.barplot(features,uniques, log=True)\n",
    "    ax.set(xlabel='Feature', ylabel='log(unique count)', title=i)\n",
    "    for p, uniq in zip(ax.patches, uniques):\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 10,\n",
    "                uniq,\n",
    "                ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeviceType,DeviceInfo and ProductCD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ProductCD: product code, the product for each transaction\n",
    "“Product isn't necessary to be a real 'product' (like one item to be added to the shopping cart). It could be any kind of service.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview(train[['ProductCD']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "i='ProductCD'\n",
    "#train[i]=train[i].fillna('Missing')\n",
    "features = list(train[i].unique())\n",
    "uniques = [(train[i]==col).sum() for col in features]\n",
    "sns.set(font_scale=1)\n",
    "ax = sns.barplot(features,uniques, log=True)\n",
    "ax.set(xlabel='Feature', ylabel='log(unique count)', title=i)\n",
    "for p, uniq in zip(ax.patches, uniques):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 10,\n",
    "                uniq,\n",
    "                ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to find out what exactly these features mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview(train[['DeviceType']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "i='DeviceType'\n",
    "train[i]=train[i].fillna('Missing')\n",
    "features = list(train[i].unique())\n",
    "uniques = [(train[i]==col).sum() for col in features]\n",
    "sns.set(font_scale=1)\n",
    "ax = sns.barplot(features,uniques, log=True)\n",
    "ax.set(xlabel='Feature', ylabel='log(unique count)', title=i)\n",
    "for p, uniq in zip(ax.patches, uniques):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 10,\n",
    "                uniq,\n",
    "                ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of data missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview(train[['DeviceInfo']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "i='DeviceInfo'\n",
    "train[i]=train[i].fillna('Missing')\n",
    "features = train['DeviceInfo'].value_counts()[:10].index.tolist()\n",
    "uniques = [(train[i]==col).sum() for col in features]\n",
    "sns.set(font_scale=1)\n",
    "ax = sns.barplot(features,uniques, log=True)\n",
    "ax.set(xlabel='Feature', ylabel='log(unique count)', title=i)\n",
    "for p, uniq in zip(ax.patches, uniques):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 10,\n",
    "                uniq,\n",
    "                ha=\"center\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['DeviceInfo'].value_counts()[:10].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a lot of missing data but as expected windows is most frequently used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card1-card6 and M1-M9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> card1 - card6: payment card information, such as card type, card category, issue bank, country, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> M1-M9: match, such as names on card and address, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = ['card1','card2','card3','card4','card5','card6','M1','M2','M3','M4','M5','M6','M7','M8','M9']\n",
    "for i in new:\n",
    "    plt.figure(figsize=(25,10))\n",
    "\n",
    "    train[i]=train[i].fillna('Missing')\n",
    "    features = list(train[i].unique()[:20])\n",
    "    #if you want to see 10 most frequent values \n",
    "    #features = train['DeviceInfo'].value_counts()[:10].index.tolist()\n",
    "    uniques = [(train[i]==col).sum() for col in features]\n",
    "    sns.set(font_scale=2)\n",
    "    ax = sns.barplot(features,uniques, log=True)\n",
    "    ax.set(xlabel='Feature', ylabel='log(unique count)', title=i)\n",
    "    for p, uniq in zip(ax.patches, uniques):\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 10,\n",
    "                uniq,\n",
    "                ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### addr1, addr2, P_emaildomain and R_emaildomain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find this 4 features important because I think they will play big role in our predictive model. Why do I think that? Because I find them very interesting and think that we can extract some key insights from them about some fraud \"patterns\" and maybe connect them to some other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">addr: address; \n",
    ">“both addresses are for purchaser: addr1 as billing region\n",
    "addr2 as billing country”\"\n",
    "\n",
    "> P_ and (R_) emaildomain: purchaser and recipient email domain; “ certain transactions don't need recipient, so Remaildomain is null.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview(train[['addr1','addr2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analize(train[['addr1','addr2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see we have 332 billing regions and 74 billing countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['addr1','addr2']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train['addr2']==87).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find this very interesting. Could it be that value 87 == USA? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analize(train[['P_emaildomain','R_emaildomain']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview(train[['P_emaildomain','R_emaildomain']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most frequently used emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['P_emaildomain','R_emaildomain']:\n",
    "    plt.figure(figsize=(25,10))\n",
    "\n",
    "    train[i]=train[i].fillna('Missing')\n",
    "    #features = list(train[i].unique()[:20])\n",
    "    #if you want to see 10 most frequent values \n",
    "    features = train[i].value_counts()[:10].index.tolist()\n",
    "    uniques = [(train[i]==col).sum() for col in features]\n",
    "    sns.set(font_scale=2)\n",
    "    ax = sns.barplot(features,uniques, log=True)\n",
    "    ax.set(xlabel='Feature', ylabel='log(unique count)', title=i+' most frequent email adresses')\n",
    "    for p, uniq in zip(ax.patches, uniques):\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 10,\n",
    "                uniq,\n",
    "                ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 least frequently used emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['P_emaildomain','R_emaildomain']:\n",
    "    plt.figure(figsize=(25,10))\n",
    "\n",
    "    train[i]=train[i].fillna('Missing')\n",
    "    #features = list(train[i].unique()[:20])\n",
    "    #if you want to see 10 most frequent values \n",
    "    features = train[i].value_counts()[-10:].index.tolist()\n",
    "    uniques = [(train[i]==col).sum() for col in features]\n",
    "    sns.set(font_scale=2)\n",
    "    ax = sns.barplot(features,uniques, log=True)\n",
    "    ax.set(xlabel='Feature', ylabel='log(unique count)', title=i+' least frequent email adresses')\n",
    "    for p, uniq in zip(ax.patches, uniques):\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 10,\n",
    "                uniq,\n",
    "                ha=\"center\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['P_emaildomain','R_emaildomain']:\n",
    "    plt.figure(figsize=(25,10))\n",
    "\n",
    "    #train[i]=train[i].fillna('Missing')\n",
    "    #features = list(train[i].unique()[:20])\n",
    "    #if you want to see 10 most frequent values \n",
    "    features = (train[train.iloc[:]['addr2']== 87]['P_emaildomain']).value_counts(sort=True)[:10].index.tolist()\n",
    "    uniques = [(train[i]==col).sum() for col in features]\n",
    "    sns.set(font_scale=2)\n",
    "    ax = sns.barplot(features,uniques, log=True)\n",
    "    ax.set(xlabel='Feature', ylabel='log(unique count)', title=i+' with addr2==87')\n",
    "    for p, uniq in zip(ax.patches, uniques):\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 10,\n",
    "                uniq,\n",
    "                ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it doesn't tell me anything special. Second interesting method some used in this competition was looking at the decimal places of the transaction amount and then looking at those mails. Why is this a smart approach? Because they saw that rows that had 3 or more decimal places were linked to non-USA emails due to the exchange of the currencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
